#+TITLE: music2score — Developer Documentation (Audio→Stems→MIDI→Lyrics→Score Pipeline)
#+AUTHOR: YAMASHITA, Takao
#+OPTIONS: toc:3 num:nil
#+PROPERTY: header-args :noweb no-export :mkdirp yes

* Overview
本プロジェクト *music2score* は「ひとつの音源から、ステム→MIDI→歌詞→スコア→PDF」を生成する総合パイプラインです。

#+begin_quote
Demucs → Ensemble → Basic Pitch → faster-whisper → music21 → MuseScore CLI
#+end_quote

開発者が読むことを前提に、以下の観点で本 README.org を再編成しています。

- 可視化重視（Mermaid フローチャート / シーケンス図）
- 責務ごとの整理
- データ構造とアルゴリズムの明確化
- 拡張ポイントの明示
- env/ 以下の実コードを完全掲載（tangle 可）

環境は以下の 3 種類をサポートします。

- macOS
- WSL2 (Ubuntu)
- Docker / Docker Compose

共通ロジックは =env/common/= に集約し、環境依存部分は個別の env 下に配置します。

* Glossary
本プロジェクトで使用する専門語を *AI モデル / パイプライン工程 / ディレクトリ構造 / パラメータ* の 4 章に整理しました。

** AI モデル・ライブラリ
| 用語                     | 説明                                                       |
|--------------------------+------------------------------------------------------------|
| *Demucs*                 | Meta Research の source separation モデル。ステム分離を担当。 |
| *htdemucs / htdemucs_6s* | Demucs 改良系列。6s は 6-stems 版。                        |
| *Basic Pitch*            | Spotify 製の Audio→MIDI 変換モデル。                       |
| *faster-whisper*         | Whisper 高速推論。word-level timestamps を取得。            |
| *music21*                | スコア操作ライブラリ。MusicXML 生成に使用。                 |
| *MuseScore CLI (mscore)* | MusicXML → PDF のレンダリングツール。                      |

** パイプライン工程
| 工程名                  | 説明                                                              |
|-------------------------+-------------------------------------------------------------------|
| *正規化 (normalize)*    | ffmpeg による音源フォーマット統一（44.1kHz / stereo / s16）。      |
| *ステム分離*            | Demucs により楽器別チャンネルへ分離。                             |
| *アンサンブル (ensemble)* | 複数モデルの出力を median / trimmed mean で統合。                  |
| *trimmed mean (tmean)*  | 外れ値を除外した平均。                                           |
| *ASR*                   | faster-whisper による歌詞抽出。                                  |
| *MusicXML 合成*          | MIDI と歌詞 JSON を music21 で統合。                             |

** ディレクトリ構造
| ディレクトリ                | 内容                                    |
|----------------------------+-----------------------------------------|
| *env/common/*              | 共通スクリプト・依存ファイル。          |
| *env/{macOS,wsl2,docker}/* | 環境別セットアップ・実行。              |
| *input/*                   | 入力音源。                              |
| *out/tmp_audio/*           | 正規化された音源。                      |
| *out/stems/*               | Demucs の生ステム。                     |
| *out/stems_ens/*           | アンサンブル済ステム。                  |
| *out/midi/*                | Basic Pitch の MIDI。                   |
| *out/lyrics/*              | faster-whisper の JSON。                |
| *out/score/*               | MusicXML と PDF。                       |

** パラメータ
| パラメータ名         | 内容                                                     |
|----------------------+----------------------------------------------------------|
| *MODELS*             | Demucs に使用するモデル名の一覧。                       |
| *ENSEMBLE_METHOD*    | median / tmean。                                         |
| *TMEAN_ALPHA*        | trimmed mean のカット割合。                              |
| *MSCORE_BIN*         | MuseScore CLI のパス。                                   |
| *ASR_SRC*            | ASR に使用する WAV（通常は vocals）。                    |
| *LYRICS_JSON*        | 歌詞 JSON の出力パス。                                   |

* 想定ディレクトリ構成

#+begin_src text
  project-root/
    README.org
    input/
      foo.wav
    out/
      tmp_audio/
      stems/
      stems_ens/
      midi/
      lyrics/
      score/
    env/
      common/
        requirements.txt
        scripts/
          process.sh
          ensemble_stems.py
          align_musicxml.py
          asr.py
      macOS/
        Makefile
      wsl2/
        Makefile
      docker/
        Dockerfile
        Makefile
        docker-compose.yml
#+end_src

* 全体パイプライン

** フローチャート
#+begin_src mermaid
  flowchart TD
    A[Input Audio] --> B[Normalize<br/>ffmpeg]
    B --> C[Demucs<br/>Model 1..N]
    C --> D[Ensemble<br/>median/tmean]
    D --> E[Stems<br/>vocals,bass,...]
    E --> F[Basic Pitch<br/>MIDI]
    F --> G[MIDI Files]
    E --> H[ASR Input Select]
    H --> I[faster-whisper]
    I --> J[Lyrics JSON]
    G --> K[music21 Score Build]
    J --> K
    K --> L[MusicXML]
    L --> M[MuseScore CLI (optional)]
    M --> N[PDF Score]
#+end_src

* 共通環境: env/common

** requirements.txt
#+begin_src conf :tangle env/common/requirements.txt
  demucs>=4.0.0
  torch
  torchaudio==2.3.1

  basic-pitch==0.4.0
  numpy==1.26.4
  soundfile==0.12.1
  librosa==0.10.2

  faster-whisper==1.0.3
  music21==9.1.0

  scikit-learn<=1.5.1
  tqdm==4.66.5
#+end_src

** process.sh
#+begin_src sh :tangle env/common/scripts/process.sh :shebang "#!/usr/bin/env bash"
  set -euo pipefail
  export TORCHAUDIO_USE_SOUNDFILE=1

  # === Args ===
  AUDIO_IN="${1:-}"
  : "${AUDIO_IN:?Usage: $0 <audio.(wav|aif|aiff|aifc|mp3|flac)>}"

  # === Paths ===
  SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
  WORK_DIR="$(cd "$SCRIPT_DIR/../.." && pwd)"   # project-root
  OUT_DIR="$WORK_DIR/out"
  TMP_DIR="$OUT_DIR/tmp_audio"
  STEM_ROOT="$OUT_DIR/stems"
  ENS_DIR="$OUT_DIR/stems_ens"
  MIDI_DIR="$OUT_DIR/midi"
  LYRICS_DIR="$OUT_DIR/lyrics"
  SCORE_DIR="$OUT_DIR/score"
  SCORE_XML="$SCORE_DIR/score.musicxml"
  SCORE_PDF="$SCORE_DIR/score.pdf"

  MODELS="${MODELS:-htdemucs}"  # e.g. "htdemucs htdemucs_6s"
  ENSEMBLE_METHOD="${ENSEMBLE_METHOD:-median}" # median | tmean
  TMEAN_ALPHA="${TMEAN_ALPHA:-0.1}"            # used when ENSEMBLE_METHOD=tmean

  mkdir -p "$OUT_DIR" "$TMP_DIR" "$STEM_ROOT" "$ENS_DIR" \
    "$MIDI_DIR" "$LYRICS_DIR" "$SCORE_DIR"

  # === 0) Normalize input ===
  BASE="$(basename "$AUDIO_IN")"
  BASE_NOEXT="${BASE%.*}"
  WAV_IN="$TMP_DIR/${BASE_NOEXT}.wav"

  echo "[normalize] $AUDIO_IN -> $WAV_IN"
  ffmpeg -y -i "$AUDIO_IN" -ac 2 -ar 44100 -sample_fmt s16 \
    "$WAV_IN" >/dev/null 2>&1

  # === 1) MODELS parsing ===
  MODELS_SANE="$(printf "%s" "$MODELS" | tr '、,' ' ' | tr -s '[:space:]' ' ')"
  read -r -a MODEL_ARR <<< "$MODELS_SANE"
  if [ "${#MODEL_ARR[@]}" -eq 0 ]; then
    MODEL_ARR=(htdemucs)
  fi
  echo "[models] parsed: ${#MODEL_ARR[@]} -> ${MODEL_ARR[*]}"

  # === 2) Demucs per-model ===
  for m in "${MODEL_ARR[@]}"; do
    echo "[demucs] model=$m"
    OUT_SUBDIR="$STEM_ROOT/$m"
    mkdir -p "$OUT_SUBDIR"
    echo "  -> demucs -n $m -o $OUT_SUBDIR $WAV_IN"
    demucs -n "$m" -o "$OUT_SUBDIR" "$WAV_IN"
  done

  # === 3) Ensemble (if >=2 models) ===
  if [ "${#MODEL_ARR[@]}" -ge 2 ]; then
    echo "[ensemble] models=${MODEL_ARR[*]} method=$ENSEMBLE_METHOD alpha=$TMEAN_ALPHA"
    python "$SCRIPT_DIR/ensemble_stems.py" \
      --models "$MODELS_SANE" \
      --stem-root "$STEM_ROOT" \
      --track "$BASE_NOEXT" \
      --out-dir "$ENS_DIR" \
      --method "$ENSEMBLE_METHOD" \
      --tmean-alpha "$TMEAN_ALPHA"
    STEM_DIR="$ENS_DIR/$BASE_NOEXT"
  else
    STEM_DIR="$STEM_ROOT/${MODEL_ARR[0]}/$BASE_NOEXT"
  fi

  echo "[stems] using STEM_DIR=$STEM_DIR"

  # === 4) Basic Pitch (per stem) ===
  mkdir -p "$MIDI_DIR"

  bp_conv() {
    local f="$1"
    if [ -f "$f" ]; then
      echo "[basic-pitch] $f"
      basic-pitch \
        --save-midi \
        --no-melodia \
        "$MIDI_DIR" \
        "$f" >/dev/null 2>&1 || true
    fi
  }

  for stem in vocals bass drums other guitar piano; do
    bp_conv "$STEM_DIR/$stem.wav"
  done

  # === 5) ASR (faster-whisper) ===
  LYRICS_JSON="$LYRICS_DIR/lyrics_words.json"
  ASR_SRC="$STEM_DIR/vocals.wav"
  [ ! -f "$ASR_SRC" ] && ASR_SRC="$WAV_IN"

  echo "[asr] source=$ASR_SRC"
  PYTHONIOENCODING=utf-8 \
  ASR_SRC="$ASR_SRC" \
  LYRICS_JSON="$LYRICS_JSON" \
  python "$SCRIPT_DIR/asr.py"

  # === 6) MusicXML ===
  echo "[xml] building MusicXML -> $SCORE_XML"
  python "$SCRIPT_DIR/align_musicxml.py" \
    --midi_dir "$MIDI_DIR" \
    --lyrics_json "$LYRICS_JSON" \
    --output "$SCORE_XML" \
    --tempo 120

  # === 7) PDF (MuseScore CLI; MSCORE_BIN は環境側で設定) ===
  if [ -n "${MSCORE_BIN:-}" ]; then
    echo "[pdf] $MSCORE_BIN $SCORE_XML -> $SCORE_PDF"
    "$MSCORE_BIN" "$SCORE_XML" -o "$SCORE_PDF" >/dev/null 2>&1 || \
      echo "[warn] PDF export failed."
  else
    echo "[info] MSCORE_BIN not set; skipped PDF export."
  fi

  echo "DONE:"
  echo " - Stems    : $STEM_DIR"
  echo " - MusicXML : $SCORE_XML"
  [ -n "${MSCORE_BIN:-}" ] && echo " - PDF      : $SCORE_PDF"
#+end_src

** ensemble_stems.py
#+begin_src python :tangle env/common/scripts/ensemble_stems.py :shebang "#!/usr/bin/env python3"
  import argparse
  from pathlib import Path
  from typing import List

  import numpy as np
  import soundfile as sf

  STEMS = ["vocals", "bass", "drums", "other", "guitar", "piano"]

  def load_stereo(path: Path):
      """Load audio as float32 stereo."""
      if not path.exists():
          return None, None
      y, sr = sf.read(path)
      if y.ndim == 1:
          y = np.stack([y, y], axis=0)  # (2, n)
      elif y.ndim == 2:
          # soundfile: (n, ch) → (ch, n)
          if y.shape[1] <= 4:  # assume (n, ch)
              y = y.T
      else:
          raise ValueError(f"Unexpected shape {y.shape} for {path}")
      return y.astype(np.float32), sr

  def pad_to_max(waves: List[np.ndarray]) -> np.ndarray:
      """Pad all waveforms to the max length."""
      maxlen = max(w.shape[1] for w in waves)
      padded = []
      for w in waves:
          if w.shape[1] < maxlen:
              z = np.zeros((2, maxlen), dtype=w.dtype)
              z[:, : w.shape[1]] = w
              w = z
          padded.append(w)
      return np.stack(padded, axis=0)  # (k, 2, n)

  def median_fuse(waves: List[np.ndarray]) -> np.ndarray:
      stack = pad_to_max(waves)
      return np.median(stack, axis=0)  # (2, n)

  def tmean_fuse(waves: List[np.ndarray], alpha: float) -> np.ndarray:
      stack = pad_to_max(waves)
      k = stack.shape[0]
      lo = int(alpha * k)
      hi = k - lo
      stack_sorted = np.sort(stack, axis=0)
      trimmed = stack_sorted[lo:hi]
      return trimmed.mean(axis=0)

  def main():
      ap = argparse.ArgumentParser()
      ap.add_argument("--models", required=True, help="space-separated model names")
      ap.add_argument(
          "--stem-root", required=True, help="root where model/track/stem.wav lives"
      )
      ap.add_argument(
          "--track", required=True, help="basename of track (without extension)"
      )
      ap.add_argument("--out-dir", required=True, help="output dir for ensembled stems")
      ap.add_argument("--method", default="median", choices=["median", "tmean"])
      ap.add_argument("--tmean-alpha", type=float, default=0.1)
      args = ap.parse_args()

      models = [m for m in args.models.split() if m]
      stem_root = Path(args.stem_root)
      out_track_dir = Path(args.out_dir) / args.track
      out_track_dir.mkdir(parents=True, exist_ok=True)

      print(f"[ensemble] models={models}")
      print(f"[ensemble] stem_root={stem_root}")
      print(f"[ensemble] out_dir={out_track_dir}")

      for stem in STEMS:
          cand: List[np.ndarray] = []
          sr_ref = None

          print(f"[ensemble] === Processing stem: {stem} ===")

          for m in models:
              # Pattern A: <root>/<model>/<track>/<stem>.wav
              p1 = stem_root / m / args.track / f"{stem}.wav"
              # Pattern B: <root>/<model>/<model>/<track>/<stem>.wav (Demucs 5+)
              p2 = stem_root / m / m / args.track / f"{stem}.wav"

              for p in (p1, p2):
                  y, sr = load_stereo(p)
                  if y is not None:
                      print(f"[ensemble] found: {p}")
                      if sr_ref is None:
                          sr_ref = sr
                      cand.append(y)
                      break
              else:
                  print(f"[ensemble] NOT found for model {m}: p1={p1}, p2={p2}")

          if not cand:
              print(f"[ensemble] --- No candidates for {stem}, skipping.")
              continue

          # Fuse
          if args.method == "tmean":
              y_out = tmean_fuse(cand, args.tmean_alpha)
          else:
              y_out = median_fuse(cand)

          out_path = out_track_dir / f"{stem}.wav"
          sf.write(out_path.as_posix(), y_out.T, sr_ref, subtype="PCM_16")
          print(f"[ensemble] wrote: {out_path}  (from {len(cand)} model(s))")

  if __name__ == "__main__":
      main()
#+end_src

** asr.py
#+begin_src python :tangle env/common/scripts/asr.py :shebang "#!/usr/bin/env python3"
  import json
  import os
  from pathlib import Path
  from faster_whisper import WhisperModel

  def main() -> None:
      audio = os.environ.get("ASR_SRC")
      out_json = os.environ.get("LYRICS_JSON")
      if not audio or not out_json:
          raise SystemExit("ASR_SRC / LYRICS_JSON must be set in environment.")

      model = WhisperModel("large-v3", device="cpu", compute_type="int8")
      segments, info = model.transcribe(audio, word_timestamps=True, language="ja")

      words = []
      for seg in segments:
          for w in (seg.words or []):
              words.append({
                  "start": float(w.start),
                  "end": float(w.end),
                  "text": w.word,
              })

      out_path = Path(out_json)
      out_path.parent.mkdir(parents=True, exist_ok=True)
      with out_path.open("w", encoding="utf-8") as f:
          json.dump({"lang": info.language, "words": words}, f,
                    ensure_ascii=False, indent=2)

      print(f"[asr] words={len(words)} -> {out_path}")

  if __name__ == "__main__":
      main()
#+end_src

** align_musicxml.py
#+begin_src python :tangle env/common/scripts/align_musicxml.py :shebang "#!/usr/bin/env python3"
  import argparse
  import json
  from pathlib import Path
  from typing import List, Tuple

  from music21 import converter, stream, tempo, meter, note, instrument

  STEM_ORDER = ["vocals", "bass", "drums", "other", "piano", "guitar"]

  def guess_stem_name(midipath: Path) -> str:
      name = midipath.stem.lower()
      for k in STEM_ORDER:
          if k in name:
              return k
      return name

  def load_words(json_path: Path) -> List[dict]:
      if not json_path.exists():
          return []
      data = json.loads(json_path.read_text("utf-8"))
      return data.get("words", [])

  def build_seconds_map(part: stream.Part, qpm: float) -> List[Tuple[note.Note, float, float]]:
      sec_per_quarter = 60.0 / qpm
      timeline: List[Tuple[note.Note, float, float]] = []
      cur_q = 0.0
      for el in part.flat.notesAndRests:
          dur_q = float(el.quarterLength)
          start_s = cur_q * sec_per_quarter
          end_s = (cur_q + dur_q) * sec_per_quarter
          timeline.append((el, start_s, end_s))
          cur_q += dur_q
      return timeline

  def attach_lyrics_to_part(part: stream.Part, words: List[dict], qpm: float) -> None:
      if not words:
          return
      tmap = build_seconds_map(part, qpm)
      wi = 0
      for el, s, e in tmap:
          if not isinstance(el, note.Note):
              continue
          mid = 0.5 * (s + e)
          best_idx, best_dist = None, 1e9
          for j in range(wi, min(wi + 10, len(words))):
              w = words[j]
              cs = 0.5 * (float(w["start"]) + float(w["end"]))
              d = abs(cs - mid)
              if d < best_dist:
                  best_dist = d
                  best_idx = j
          if best_idx is not None:
              el.addLyric(words[best_idx]["text"])
              wi = best_idx + 1

  def parse_midi_first_part(midi_path: Path) -> stream.Part:
      s = converter.parse(str(midi_path))
      if getattr(s, "parts", None) and len(s.parts) > 0:
          return s.parts[0]
      return s

  def main() -> None:
      ap = argparse.ArgumentParser()
      ap.add_argument("--midi_dir", required=True)
      ap.add_argument("--lyrics_json", required=True)
      ap.add_argument("--output", required=True)
      ap.add_argument("--tempo", type=float, default=120.0)
      args = ap.parse_args()

      midi_dir = Path(args.midi_dir)
      mids = sorted(midi_dir.glob("*_basic_pitch.mid"))
      if not mids:
          raise SystemExit(f"No MIDI files found in {midi_dir}")

      words = load_words(Path(args.lyrics_json))

      sc = stream.Score()
      sc.insert(0, tempo.MetronomeMark(number=args.tempo))
      sc.insert(0, meter.TimeSignature("4/4"))

      parts_added = 0
      for mpath in mids:
          stem = guess_stem_name(mpath)
          part_stream = parse_midi_first_part(mpath)
          try:
              inst = instrument.fromString(stem.capitalize())
          except Exception:
              inst = instrument.Instrument()
              inst.instrumentName = stem.capitalize()
          part_stream.insert(0, inst)
          if stem == "vocals":
              attach_lyrics_to_part(part_stream, words, qpm=args.tempo)
          sc.append(part_stream)
          parts_added += 1

      if parts_added == 0:
          raise SystemExit("No parsable MIDI parts.")

      out_path = Path(args.output)
      out_path.parent.mkdir(parents=True, exist_ok=True)
      sc.write("musicxml", fp=str(out_path))
      print(f"[xml] wrote MusicXML: {out_path}")

  if __name__ == "__main__":
      main()
#+end_src

* macOS: env/macOS
** Makefile
#+begin_src makefile :tangle env/macOS/Makefile
  VENV := .venv
  PY   := $(VENV)/bin/python
  PIP  := $(VENV)/bin/pip
  AUDIO ?= foo.wav

  MSCORE_BIN := /Applications/MuseScore\ 4.app/Contents/MacOS/mscore
  export MSCORE_BIN

  all: install

  # venv がなければ作る
  $(VENV)/bin/activate:
  	python3 -m venv $(VENV)

  install: $(VENV)/bin/activate
  	$(PY) -m pip install -U pip
  	$(PIP) install -r ../common/requirements.txt

  run: $(VENV)/bin/activate
  	# venv の bin を PATH に足してからシェルスクリプトを bash で実行
  	PATH="$(VENV)/bin:$$PATH" \
  	MODELS="htdemucs htdemucs_6s" \
  	ENSEMBLE_METHOD=tmean \
  	TMEAN_ALPHA=0.1 \
  	bash ../common/scripts/process.sh "../input/$(AUDIO)"
#+end_src

* WSL2
** Makefile
#+begin_src makefile :tangle env/wsl2/Makefile
  VENV := .venv
  PY   := $(VENV)/bin/python
  PIP  := $(VENV)/bin/pip
  AUDIO ?= foo.wav

  MSCORE_BIN := /usr/bin/mscore
  export MSCORE_BIN

  all: install

  install:
  	sudo apt update
  	sudo apt install -y ffmpeg libsndfile1 musescore
  	python3 -m venv $(VENV)
  	source $(VENV)/bin/activate
  	$(PIP) install -U pip
  	$(PIP) install -r ../common/requirements.txt

  run:
  	source $(VENV)/bin/activate
  	MODELS="htdemucs htdemucs_6s" \
  	ENSEMBLE_METHOD=median \
  	bash ../common/scripts/process.sh "../input/$(AUDIO)"
#+end_src
* Docker
** Dockerfile
#+begin_src dockerfile :tangle env/docker/Dockerfile
  FROM python:3.10-slim

  RUN apt-get update && apt-get install -y ffmpeg libsndfile1 libgl1 && \
      apt-get clean && rm -rf /var/lib/apt/lists/*

  WORKDIR /opt/music2score

  # 共通依存のみコピー
  COPY env/common/requirements.txt ./requirements.txt
  RUN pip install -U pip && pip install -r requirements.txt

  # 共通スクリプトのみコピー（ソースはイメージ内に固定）
  COPY env/common/scripts ./env/common/scripts
  RUN chmod +x env/common/scripts/process.sh

  # 入力音源はコンテナ外から /input にマウントする前提
  ENTRYPOINT ["/opt/music2score/env/common/scripts/process.sh"]
#+end_src

** Makefile
#+begin_src makefile :tangle env/docker/Makefile
  IMAGE := music2score
  AUDIO ?= foo.wav

  build:
  	docker build -t $(IMAGE) -f Dockerfile ../..

  run:
  	docker run --rm \
  	  -v ../../input:/input \
  	  -v ../../out:/out \
  	  $(IMAGE) /input/$(AUDIO)
#+end_src

** docker-compose.yml
#+begin_src yaml :tangle env/docker/docker-compose.yml
  services:
    music2score:
      build:
        context: ../..
        dockerfile: env/docker/Dockerfile
      container_name: music2score
      volumes:
        - ../../input:/input
        - ../../out:/out
      command: ["/opt/music2score/env/common/scripts/process.sh", "/input/foo.wav"]
#+end_src

* Developer Guide

** データフロー（directory-level）
#+begin_src mermaid
  flowchart LR
    A[input/foo.wav] --> B[tmp_audio]
    B --> C[stems]
    C --> D[stems_ens]
    D --> E[midi]
    D --> F[ASR src]
    F --> G[lyrics JSON]
    E --> H[MusicXML]
    G --> H
    H --> I[PDF]
#+end_src

** シーケンス図（macOS 例）
#+begin_src mermaid
  sequenceDiagram
    participant User
    participant Make
    participant SH as process.sh
    participant Dem as demucs
    participant Ens as ensemble
    participant BP as basic-pitch
    participant ASR
    participant XML
    participant MS as mscore

    User->>Make: make run
    Make->>SH: process.sh foo.wav
    SH->>Dem: run demucs (multi-model)
    Dem-->>SH: stems
    SH->>Ens: ensemble_stems.py
    Ens-->>SH: fused stems
    loop stems
      SH->>BP: basic-pitch
      BP-->>SH: midi
    end
    SH->>ASR: asr.py
    ASR-->>SH: lyrics_words.json
    SH->>XML: align_musicxml.py
    XML-->>SH: score.musicxml
    SH->>MS: mscore -o score.pdf
    MS-->>SH: score.pdf
    SH-->>User: DONE
#+end_src

* 拡張ポイント

** 追加しやすい箇所（表）
| 項目 | 変更地点 | 備考 |
|------+-----------+-------|
| Demucs モデル追加 | Makefile の MODELS | ensemble_stems.py 修正不要 |
| アンサンブル手法 | ENSEMBLE_METHOD | median/tmean |
| ステム種追加 | ensemble_stems.py, process.sh, align_musicxml.py | 3 箇所必須 |
| 歌詞推定モデル変更 | asr.py | Whisper variant など |
| 拍子・テンポ可変 | align_musicxml.py | 3/4, ritardando 等 |

* トラブルシュート
** Demucs が落ちる
- モデルを軽量化
- 正規化のサンプリングレート変更

** Basic Pitch が MIDI を出さない
- ログ有効化
- 無音区間が長い可能性

** lyrics_words.json が空
- ASR_SRC の WAV を確認

** MusicXML が生成されない
- Basic Pitch の *_basic_pitch.mid があるか確認

* 使い方（User 向け）
1. Org 保存 → =org-babel-tangle=
2. macOS:
   #+begin_src sh
     cd env/macOS
     make install
     make run
   #+end_src
3. WSL2:
   #+begin_src sh
     cd env/wsl2
     make install
     make run
   #+end_src
4. Docker:
   #+begin_src sh
     cd env/docker
     make build
     make run
   #+end_src

* Appendix: music2score 開発ポリシー
- 共通ロジックは *env/common* に集約
- 環境依存は *env/macOS, env/wsl2, env/docker*
- Mermaid 図は org-babel-preview で視覚化
- tangle → 再現性のあるビルド

以上で、開発者が迷わずコードを追跡できる README.org 完全版となります。
