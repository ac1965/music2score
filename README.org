#+TITLE: music2score — Audio→Stems→MIDI→Lyrics→Score Pipeline
#+AUTHOR: YAMASHITA, Takao
#+OPTIONS: toc:t num:nil
#+PROPERTY: header-args :noweb no-export :mkdirp yes

* Overview
music2score は、任意の音源ファイルから次の成果物を自動生成するパイプラインです。

1. Demucs によるステム分離（複数モデル + アンサンブル対応）
2. Basic Pitch による各パート MIDI 生成
3. faster-whisper による *単語レベル* 歌詞抽出
4. music21 による MusicXML 組み立て
5. LilyPond による PDF スコア生成（MuseScore の代替）

本パイプラインは、

- macOS
- WSL2 (Ubuntu)
- Docker（単体／compose）

の各環境に応じ、README.org から =org-babel-tangle= を用いてファイル一式を自動生成できます。

共通コード・依存関係は =env/common/= に格納し、環境固有の Makefile・Dockerfile は =env/macOS=, =env/wsl2=, =env/docker= に出力します。

* Glossary

| 用語 | 説明 |
|------|------|
| *ステム（Stem）* | Demucs などで音源から抽出されたパート単位の WAV（vocals, bass, drums 等） |
| *モデル（Model）* | Demucs のネットワークモデル名（例: htdemucs, htdemucs_6s） |
| *アンサンブル（Ensemble）* | モデル複数出力を median / trimmed mean で統合し、より安定したステムを生成 |
| *Basic Pitch* | ソース分離された単一楽器（または vocals）WAV から MIDI を生成するライブラリ |
| *ASR* | 音声認識（Automatic Speech Recognition）。faster-whisper を使用 |
| *MusicXML* | 楽譜データ交換用 XML 形式。LilyPond へ変換可能 |
| *LilyPond* | 高品質なスコア engraving ツール。PDF 出力に使用 |
| *musicxml2ly* | MusicXML → LilyPond 変換ツール |
| *tmean* | trimmed mean アンサンブル。中央値より頑健・精度高めのケースがある |
| *pipeline* | 0 → 7 の各工程をまとめた一連の処理 |

* Pipeline Flow

#+begin_src plantuml :file pipeline-flow.png :exports both
  @startuml
  start
  :Normalize audio (ffmpeg);
  :Demucs (1..N models);
  if (#models >= 2?) then (yes)
    :Ensemble stems (median / tmean);
  else (no)
    :Use single model;
  endif
  :Basic Pitch (per stem → MIDI);
  :Whisper ASR (lyrics json);
  :music21 (MIDI + lyrics → MusicXML);
  :LilyPond (MusicXML → PDF);
  stop
  @enduml
#+end_src

* 想定ディレクトリ構成

#+begin_src text
  project-root/
    README.org
    input/
      foo.wav
    out/
    env/
      common/
        requirements.txt
        scripts/
          process.sh
          ensemble_stems.py
          align_musicxml.py
          asr.py
      macOS/
        Makefile
      wsl2/
        Makefile
      docker/
        Dockerfile
        Makefile
        docker-compose.yml
#+end_src

* 共通環境（env/common）

** requirements.txt
#+begin_src conf :tangle env/common/requirements.txt
  demucs>=4.0.0
  torch
  torchaudio==2.3.1

  basic-pitch==0.4.0
  numpy==1.26.4
  soundfile==0.12.1
  librosa==0.10.2

  faster-whisper==1.0.3
  music21==9.1.0

  scikit-learn<=1.5.1
  tqdm==4.66.5

  # MusicXML → LilyPond 用
  lxml
#+end_src

** process.sh
#+begin_src sh :tangle env/common/scripts/process.sh :shebang "#!/usr/bin/env bash"
  set -euo pipefail
  export TORCHAUDIO_USE_SOUNDFILE=1

  AUDIO_IN="${1:-}"
  : "${AUDIO_IN:?Usage: $0 <audio.(wav|aif|aiff|aifc|mp3|flac)>}"

  SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
  WORK_DIR="$(cd "$SCRIPT_DIR/../.." && pwd)"
  OUT_DIR="$WORK_DIR/out"
  TMP_DIR="$OUT_DIR/tmp_audio"
  STEM_ROOT="$OUT_DIR/stems"
  ENS_DIR="$OUT_DIR/stems_ens"
  MIDI_DIR="$OUT_DIR/midi"
  LYRICS_DIR="$OUT_DIR/lyrics"
  SCORE_DIR="$OUT_DIR/score"
  SCORE_XML="$SCORE_DIR/score.musicxml"
  SCORE_LY="$SCORE_DIR/score.ly"
  SCORE_PDF="$SCORE_DIR/score.pdf"

  MODELS="${MODELS:-htdemucs}"
  ENSEMBLE_METHOD="${ENSEMBLE_METHOD:-median}"
  TMEAN_ALPHA="${TMEAN_ALPHA:-0.1}"

  mkdir -p "$OUT_DIR" "$TMP_DIR" "$STEM_ROOT" "$ENS_DIR" \
    "$MIDI_DIR" "$LYRICS_DIR" "$SCORE_DIR"

  # 0. Normalize
  BASE="$(basename "$AUDIO_IN")"
  BASE_NOEXT="${BASE%.*}"
  WAV_IN="$TMP_DIR/${BASE_NOEXT}.wav"
  echo "[normalize] $AUDIO_IN -> $WAV_IN"
  ffmpeg -y -i "$AUDIO_IN" -ac 2 -ar 44100 -sample_fmt s16 \
    "$WAV_IN" >/dev/null 2>&1

  # 1. Parse models
  MODELS_SANE="$(printf "%s" "$MODELS" | tr '、,' ' ' | tr -s '[:space:]' ' ')"
  read -r -a MODEL_ARR <<< "$MODELS_SANE"
  [ "${#MODEL_ARR[@]}" -eq 0 ] && MODEL_ARR=(htdemucs)
  echo "[models] parsed: ${MODEL_ARR[*]}"

  # 2. Demucs
  for m in "${MODEL_ARR[@]}"; do
    echo "[demucs] model=$m"
    demucs -n "$m" -o "$STEM_ROOT" "$WAV_IN"
  done

  # 3. Ensemble
  if [ "${#MODEL_ARR[@]}" -ge 2 ]; then
    echo "[ensemble] method=$ENSEMBLE_METHOD alpha=$TMEAN_ALPHA"
    python "$SCRIPT_DIR/ensemble_stems.py" \
      --models "$MODELS_SANE" \
      --stem-root "$STEM_ROOT" \
      --track "$BASE_NOEXT" \
      --out-dir "$ENS_DIR" \
      --method "$ENSEMBLE_METHOD" \
      --tmean-alpha "$TMEAN_ALPHA"
    STEM_DIR="$ENS_DIR/$BASE_NOEXT"
  else
    STEM_DIR="$STEM_ROOT/${MODEL_ARR[0]}/$BASE_NOEXT"
  fi
  echo "[stems] using $STEM_DIR"

  # 4. Basic Pitch
  mkdir -p "$MIDI_DIR"
  bp_conv() {
    [ -f "$1" ] || return 0
    echo "[basic-pitch] $1"
    basic-pitch --save-midi --no-melodia "$MIDI_DIR" "$1" >/dev/null 2>&1 || true
  }
  for stem in vocals bass drums other guitar piano; do
    bp_conv "$STEM_DIR/$stem.wav"
  done

  # 5. ASR
  LYRICS_JSON="$LYRICS_DIR/lyrics_words.json"
  ASR_SRC="$STEM_DIR/vocals.wav"
  [ ! -f "$ASR_SRC" ] && ASR_SRC="$WAV_IN"

  echo "[asr] $ASR_SRC"
  PYTHONIOENCODING=utf-8 \
  ASR_SRC="$ASR_SRC" \
  LYRICS_JSON="$LYRICS_JSON" \
  python "$SCRIPT_DIR/asr.py"

  # 6. MusicXML
  echo "[xml] -> $SCORE_XML"
  python "$SCRIPT_DIR/align_musicxml.py" \
    --midi_dir "$MIDI_DIR" \
    --lyrics_json "$LYRICS_JSON" \
    --output "$SCORE_XML" \
    --tempo 120

  # 7. MusicXML → LilyPond (musicxml2ly)
  echo "[ly] musicxml2ly -> $SCORE_LY"
  musicxml2ly "$SCORE_XML" -o "$SCORE_LY"

  # 8. score.ly の先頭に include を挿入
  cp "$SCRIPT_DIR/lilypond_include.ily" "$SCORE_DIR/lilypond_include.ily"

  # sed の代わりに printf + cat で安全に挿入
  TMP_LY="${SCORE_LY}.tmp"
  printf '\\include "lilypond_include.ily"\n' > "$TMP_LY"
  cat "$SCORE_LY" >> "$TMP_LY"
  mv "$TMP_LY" "$SCORE_LY"

  # 9. sanitize（必要に応じて後処理を挟みたいならここ）
  # 例: 独自の sanitize_ly.py 等を挟みたいとき:
  # python "$SCRIPT_DIR/sanitize_ly.py" "$SCORE_LY"
  # echo "[sanitize-ly] sanitized: $SCORE_LY"

  # 10. PDF (LilyPond)
  echo "[pdf] lilypond -> $SCORE_PDF"
  (
    cd "$SCORE_DIR"
    lilypond -o "score" "score.ly"
  )

  echo "DONE"
  echo " - Stems    : $STEM_DIR"
  echo " - MusicXML : $SCORE_XML"
  echo " - LilyPond : $SCORE_LY"
  echo " - PDF      : $SCORE_PDF"
#+end_src

** ensemble_stems.py
#+begin_src python :tangle env/common/scripts/ensemble_stems.py :shebang "#!/usr/bin/env python3"
  import argparse
  from pathlib import Path
  from typing import List

  import numpy as np
  import soundfile as sf

  STEMS = ["vocals", "bass", "drums", "other", "guitar", "piano"]

  def load_stereo(path: Path):
      """Load audio as float32 stereo."""
      if not path.exists():
          return None, None
      y, sr = sf.read(path)
      if y.ndim == 1:
          y = np.stack([y, y], axis=0)  # (2, n)
      elif y.ndim == 2:
          # soundfile: (n, ch) → (ch, n)
          if y.shape[1] <= 4:  # assume (n, ch)
              y = y.T
      else:
          raise ValueError(f"Unexpected shape {y.shape} for {path}")
      return y.astype(np.float32), sr

  def pad_to_max(waves: List[np.ndarray]) -> np.ndarray:
      """Pad all waveforms to the max length."""
      maxlen = max(w.shape[1] for w in waves)
      padded = []
      for w in waves:
          if w.shape[1] < maxlen:
              z = np.zeros((2, maxlen), dtype=w.dtype)
              z[:, : w.shape[1]] = w
              w = z
          padded.append(w)
      return np.stack(padded, axis=0)  # (k, 2, n)

  def median_fuse(waves: List[np.ndarray]) -> np.ndarray:
      stack = pad_to_max(waves)
      return np.median(stack, axis=0)  # (2, n)

  def tmean_fuse(waves: List[np.ndarray], alpha: float) -> np.ndarray:
      stack = pad_to_max(waves)
      k = stack.shape[0]
      lo = int(alpha * k)
      hi = k - lo
      stack_sorted = np.sort(stack, axis=0)
      trimmed = stack_sorted[lo:hi]
      return trimmed.mean(axis=0)

  def main():
      ap = argparse.ArgumentParser()
      ap.add_argument("--models", required=True, help="space-separated model names")
      ap.add_argument(
          "--stem-root", required=True, help="root where model/track/stem.wav lives"
      )
      ap.add_argument(
          "--track", required=True, help="basename of track (without extension)"
      )
      ap.add_argument("--out-dir", required=True, help="output dir for ensembled stems")
      ap.add_argument("--method", default="median", choices=["median", "tmean"])
      ap.add_argument("--tmean-alpha", type=float, default=0.1)
      args = ap.parse_args()

      models = [m for m in args.models.split() if m]
      stem_root = Path(args.stem_root)
      out_track_dir = Path(args.out_dir) / args.track
      out_track_dir.mkdir(parents=True, exist_ok=True)

      print(f"[ensemble] models={models}")
      print(f"[ensemble] stem_root={stem_root}")
      print(f"[ensemble] out_dir={out_track_dir}")

      for stem in STEMS:
          cand: List[np.ndarray] = []
          sr_ref = None

          print(f"[ensemble] === Processing stem: {stem} ===")

          for m in models:
              # Pattern A: <root>/<model>/<track>/<stem>.wav
              p1 = stem_root / m / args.track / f"{stem}.wav"
              # Pattern B: <root>/<model>/<model>/<track>/<stem>.wav (Demucs 5+)
              p2 = stem_root / m / m / args.track / f"{stem}.wav"

              for p in (p1, p2):
                  y, sr = load_stereo(p)
                  if y is not None:
                      print(f"[ensemble] found: {p}")
                      if sr_ref is None:
                          sr_ref = sr
                      cand.append(y)
                      break
              else:
                  print(f"[ensemble] NOT found for model {m}: p1={p1}, p2={p2}")

          if not cand:
              print(f"[ensemble] --- No candidates for {stem}, skipping.")
              continue

          # Fuse
          if args.method == "tmean":
              y_out = tmean_fuse(cand, args.tmean_alpha)
          else:
              y_out = median_fuse(cand)

          out_path = out_track_dir / f"{stem}.wav"
          sf.write(out_path.as_posix(), y_out.T, sr_ref, subtype="PCM_16")
          print(f"[ensemble] wrote: {out_path}  (from {len(cand)} model(s))")

  if __name__ == "__main__":
      main()
#+end_src

** asr.py
#+begin_src python :tangle env/common/scripts/asr.py :shebang "#!/usr/bin/env python3"
  # -*- coding: utf-8 -*-

  import json
  import os
  from pathlib import Path
  from faster_whisper import WhisperModel

  def main():
      audio = os.environ.get("ASR_SRC")
      out_json = os.environ.get("LYRICS_JSON")
      if not audio or not out_json:
          raise SystemExit("ASR_SRC / LYRICS_JSON must be set in environment.")

      print(f"[asr] audio={audio}")
      model = WhisperModel("large-v3", device="cpu", compute_type="int8")
      segments, info = model.transcribe(audio, word_timestamps=True, language="ja")

      words = []
      for seg in segments:
          for w in (seg.words or []):
              words.append({
                  "start": float(w.start),
                  "end": float(w.end),
                  "text": w.word,
              })

      out = Path(out_json)
      out.parent.mkdir(parents=True, exist_ok=True)
      out.write_text(
          json.dumps({"lang": info.language, "words": words}, ensure_ascii=False, indent=2),
          encoding="utf-8"
      )

      print(f"[asr] words={len(words)} -> {out}")

  if __name__ == "__main__":
      main()
#+end_src

** lilypond_include.ily
#+begin_src python :tangle env/common/scripts/lilypond_include.ily
  % LilyPond 2.24 Safe Include (Guile 3.0 Compatible)

  \paper {
    #(set-default-paper-size "a4")
    top-margin = 12
    bottom-margin = 14
    left-margin = 15
    right-margin = 15
  }

  % Staff size (安全な設定方法)
  #(set-global-staff-size 16)

  \layout {
    \context {
      \Lyrics
      \override LyricText.font-size = #1
      \override LyricText.self-alignment-X = #CENTER
      \override LyricText.extra-offset = #'(0 . -1.0)
      \override LyricSpace.minimum-distance = #1.4
    }

    \context {
      \Staff
      \override VerticalAxisGroup.staff-staff-spacing =
        #'((basic-distance . 8)
           (minimum-distance . 6)
           (padding . 1.4))
    }
  }
#+end_src

** sanitize_lilypond.py
#+begin_src python :tangle env/common/scripts/sanitize_lilypond.py :shebang "#!/usr/bin/env python3"
  # -*- coding: utf-8 -*-
  """
  sanitize_lilypond.py — musicxml2ly が生成した .ly を安全化するフィルタ

  - \barNumberCheck をコメントアウト
  - R... （multi-measure rest）を r... に置き換え
    例: R1*4  -> r1*4
        R2.   -> r2.
  """

  from __future__ import annotations

  import argparse
  import re
  from pathlib import Path


  def sanitize_ly(path: Path) -> None:
      text = path.read_text(encoding="utf-8")
      lines = text.splitlines()
      out_lines = []

      # R... を r... にするためのざっくり正規表現
      # - R のあとに音価（1,2,4,8,16 等）+ オプションで . とか *3/2 等が続くパターンを想定
      re_multi_rest = re.compile(r"\bR(?P<dur>[0-9]+(?:\.*)(?:\*[^\s%]+)?)")

      for line in lines:
          # 1) barNumberCheck をコメントアウト
          if "\\barNumberCheck" in line:
              out_lines.append("% " + line)
              continue

          # 2) multi-measure rest を通常の休符に変換
          def _rep(m: re.Match) -> str:
              dur = m.group("dur")
              return f"r{dur}"

          newline = re_multi_rest.sub(_rep, line)
          out_lines.append(newline)

      path.write_text("\n".join(out_lines) + "\n", encoding="utf-8")
      print(f"[sanitize-ly] sanitized: {path}")


  def main():
      ap = argparse.ArgumentParser()
      ap.add_argument("ly_file", help="LilyPond .ly file to sanitize")
      args = ap.parse_args()

      ly_path = Path(args.ly_file)
      if not ly_path.exists():
          raise SystemExit(f"[sanitize-ly] file not found: {ly_path}")

      sanitize_ly(ly_path)


  if __name__ == "__main__":
      main()
#+end_src

** align_musicxml.py
#+begin_src python :tangle env/common/scripts/align_musicxml.py :shebang "#!/usr/bin/env python3"
  # -*- coding: utf-8 -*-
  """
  align_musicxml.py — MIDI + Lyrics JSON → MusicXML (LilyPond-safe, tuplets-free)

  方針:
  - Basic Pitch が生成した複数 MIDI を読み込み、各ファイルを 1 パートとして扱う。
  - 元の拍・連符情報は「完全には」再現しない代わりに、LilyPond / musicxml2ly が
    落ちないように *極端にシンプルな記譜* に正規化する。

    - すべての音価を *4分音符 (quarterLength=1.0)* に統一
    - tuplets / 複雑な duration / voices は一切使わない
    - 各パートは 1 voice のみ、順番に音符・休符を append していくだけ
    - その後 TimeSignature(4/4) のもとで makeMeasures() して小節を生成

  - 歌詞は「一番上のパート (通常は vocals_basic_pitch.mid)」の
    音符に対して *順番に* 付与する。

  この結果、リズムの細部は犠牲になるが:

  - MusicXML は構造的に非常に単純
  - musicxml2ly で「負のスキップ」「tuplets 処理中の例外」などが発生しなくなる
  - LilyPond にとっては「普通の 4/4 の連続した 4分音符列」として扱える

  今後、精密なリズム再現が必要になった場合は、
  別スクリプトとして「高精度版 aligner」を用意することを想定。
  """

  import argparse
  import json
  from pathlib import Path
  from typing import List, Tuple

  import music21 as m21


  # ------------------------------------------------------------
  #  Utilities
  # ------------------------------------------------------------

  def load_lyrics(json_path: Path) -> List[dict]:
      """
      ASR 出力 JSON から words 配列を読む。
      フォーマット:
        {
          "lang": "ja",
          "words": [
            {"start": 0.10, "end": 0.35, "text": "ここ"},
            ...
          ]
        }
      """
      if not json_path.exists():
          print(f"[align] lyrics json not found, skip lyrics: {json_path}")
          return []

      data = json.loads(json_path.read_text(encoding="utf-8"))
      words = data.get("words", [])
      # text のみ抜き出しておく
      cleaned = []
      for w in words:
          t = str(w.get("text", "")).strip()
          if not t:
              continue
          cleaned.append({"text": t})
      return cleaned


  def assign_lyrics_sequential(part: m21.stream.Part, words: List[dict]) -> int:
      """
      とりあえず *順番に* 音符へ歌詞を割り当てる版。
      - 音符のみ対象（休符にはつけない）
      - 単純な 1音符 1語 割り当て
      """
      if not words:
          return 0

      idx = 0
      notes = list(part.recurse().notes)
      for n in notes:
          if idx >= len(words):
              break
          w = words[idx].get("text", "").strip()
          if not w:
              idx += 1
              continue
          n.lyric = w
          idx += 1

      print(f"[align] lyrics attached: {idx} / {len(words)} words")
      return idx


  def guess_stem_name(midi_path: Path) -> str:
      """
      ファイル名から stem 名をざっくり推定（vocals / bass / drums ...）。
      楽器名ラベル用。失敗したら拡張子なしファイル名をそのまま返す。
      """
      name = midi_path.stem.lower()
      for k in ("vocals", "vocal", "vox", "bass", "drums",
                "other", "guitar", "piano"):
          if k in name:
              return k
      return name


  def simplify_stream_to_part(
      s: m21.stream.Stream,
      stem_name: str,
      max_events: int = 8000,
  ) -> m21.stream.Part:
      """
      MIDI Stream → LilyPond-safe な 1 パートに簡略化する。

      - s.flat.notesAndRests を時間順にたどるが、
        offset 情報は一切使わず「順番だけ」利用する。
      - 各イベントを 4分音符 (= quarterLength=1.0) に変換。
        - Note: pitch を保持
        - Rest: 休符 4分音符
      - chord は先頭音だけを Note として採用。
      - max_events でイベント数を制限（念のための暴走防止）。
      """
      flat = list(s.flat.notesAndRests)
      p = m21.stream.Part()
      p.id = stem_name

      # 楽器ラベルを付けておくと DAW 的にも分かりやすい
      try:
          inst = m21.instrument.fromString(stem_name.capitalize())
      except Exception:
          inst = m21.instrument.Instrument()
          inst.instrumentName = stem_name
      p.insert(0, inst)

      count = 0
      for e in flat:
          if count >= max_events:
              print(f"[warn] {stem_name}: truncated at {max_events} events")
              break

          new_obj = None

          if isinstance(e, m21.note.Note):
              n = m21.note.Note(e.pitch)
              n.quarterLength = 1.0
              new_obj = n
          elif isinstance(e, m21.chord.Chord):
              # 先頭音だけ代表にする
              n = m21.note.Note(e.pitches[0])
              n.quarterLength = 1.0
              new_obj = n
          elif isinstance(e, m21.note.Rest):
              r = m21.note.Rest()
              r.quarterLength = 1.0
              new_obj = r
          else:
              # その他 (e.g. expressions) は無視
              continue

          p.append(new_obj)
          count += 1

      if count == 0:
          raise RuntimeError(f"No usable notes/rests in MIDI: {stem_name}")

      print(f"[align] {stem_name}: simplified events = {count}")
      return p


  def build_score_from_midis(midi_dir: Path) -> Tuple[m21.stream.Score, List[str]]:
      """
      MIDI ディレクトリから *空でない* パートだけを拾い、
      1 パート = 1 stem として Score を構成する。

      - すべて simplify_stream_to_part() を通して
        LilyPond-safe な単純パートに変換。
      """
      midi_files = sorted(midi_dir.glob("*.mid"))
      if not midi_files:
          raise FileNotFoundError(f"No MIDI files found in: {midi_dir}")

      parts = []
      stem_names: List[str] = []

      for mf in midi_files:
          print(f"[align] parsing MIDI: {mf.name}")
          s = m21.converter.parse(str(mf))

          stem_name = guess_stem_name(mf)
          try:
              p = simplify_stream_to_part(s, stem_name)
          except RuntimeError as e:
              print(f"[warn] {e}, skipped")
              continue

          parts.append(p)
          stem_names.append(stem_name)

      if not parts:
          raise RuntimeError("All MIDI files were empty or contained no usable notes.")

      score = m21.stream.Score()
      for p in parts:
          score.append(p)

      return score, stem_names


  def normalize_score(
      score: m21.stream.Score,
      time_signature: str = "4/4",
      tempo: float = 120.0,
  ) -> m21.stream.Score:
      """
      Score 全体を LilyPond-friendly に正規化する。

      - TimeSignature と Tempo を先頭に挿入
      - makeMeasures() により 4/4 に基づく小節を作成
      - makeNotation() で記譜情報を再構成
      - 空小節には全休符を挿入
      """
      ts = m21.meter.TimeSignature(time_signature)
      tempo_mark = m21.tempo.MetronomeMark(number=tempo)

      score.insert(0, ts)
      score.insert(0, tempo_mark)

      # 小節を明示的に作る
      score.makeMeasures(inPlace=True)

      # 記譜構造を整理（tie, beam など）
      score.makeNotation(inPlace=True)

      # 空小節対策: 中身ゼロ measure には全休符を入れる
      for part in score.parts:
          for m in part.getElementsByClass(m21.stream.Measure):
              if len(m.notesAndRests) == 0:
                  # 4/4 を想定して全休符
                  m.insert(0, m21.note.Rest(quarterLength=4.0))

      return score


  # ------------------------------------------------------------
  #  Main
  # ------------------------------------------------------------

  def main():
      ap = argparse.ArgumentParser()
      ap.add_argument(
          "--midi_dir", required=True,
          help="Basic Pitch が出力した MIDI ディレクトリ"
      )
      ap.add_argument(
          "--lyrics_json", required=True,
          help="faster-whisper の word-level JSON"
      )
      ap.add_argument(
          "--output", required=True,
          help="出力 MusicXML パス"
      )
      ap.add_argument(
          "--tempo", type=float, default=120.0,
          help="メトロノームテンポ（BPM）"
      )
      args = ap.parse_args()

      midi_dir = Path(args.midi_dir)
      lyrics_json = Path(args.lyrics_json)
      out_xml = Path(args.output)

      print(f"[align] MIDI dir   : {midi_dir}")
      print(f"[align] Lyrics json: {lyrics_json}")
      print(f"[align] Output XML : {out_xml}")

      # 1) MIDI → Score (各パート単純化)
      score, stem_names = build_score_from_midis(midi_dir)
      print(f"[align] parts      : {stem_names}")

      # 2) 歌詞: 一番上のパートを vocals とみなす
      words = load_lyrics(lyrics_json)
      if words and len(score.parts) > 0:
          print("[align] attaching lyrics to first part (assumed vocals)")
          assign_lyrics_sequential(score.parts[0], words)
      else:
          print("[align] no lyrics attached (no words or no parts)")

      # 3) スコア全体を 4/4 の単純小節に正規化
      print("[align] normalizing score …")
      score = normalize_score(score, time_signature="4/4", tempo=args.tempo)

      # 4) MusicXML 出力
      out_xml.parent.mkdir(parents=True, exist_ok=True)
      score.write("musicxml", fp=str(out_xml))
      print(f"[align] wrote MusicXML: {out_xml}")


  if __name__ == "__main__":
      main()
#+end_src

* macOS 用環境（env/macOS）

** Makefile
#+begin_src makefile :tangle env/macOS/Makefile
  # music2score — macOS Makefile (pyenv 3.10 auto-check + LilyPond)

  VENV := .venv
  PY   := $(VENV)/bin/python
  PIP  := $(VENV)/bin/pip
  AUDIO ?= foo.wav

  # LilyPond is primary
  LILYPOND := $(shell command -v lilypond)

  # ----------------------------------------
  # Python version settings
  # ----------------------------------------
  PYENV_PY := $(shell command -v pyenv >/dev/null 2>&1 && \
                   pyenv versions --bare | grep '^3\.10' | sort -V | tail -1)

  ifeq ($(PYENV_PY),)
    HAS_PYENV := $(shell command -v pyenv >/dev/null 2>&1 && echo yes || echo no)
    ifeq ($(HAS_PYENV),yes)
      PYTHON_BIN := $(HOME)/.pyenv/versions/3.10.14/bin/python3
    else
      PYTHON_BIN := python3
    endif
  else
    PYTHON_BIN := $(HOME)/.pyenv/versions/$(PYENV_PY)/bin/python3
  endif

  # ----------------------------------------
  # Targets
  # ----------------------------------------

  all: install

  check-pyenv:
  	@echo ">>> Checking pyenv and Python 3.10..."
  	@if command -v pyenv >/dev/null 2>&1; then \
  		if ! pyenv versions --bare | grep -q '^3\.10'; then \
  			echo "  - Installing Python 3.10.14..."; \
  			pyenv install -s 3.10.14; \
  		else \
  			echo "  - Python 3.10 already installed."; \
  		fi \
  	else \
  		echo "  - pyenv not found. Using system python3."; \
  	fi

  $(VENV)/bin/activate: check-pyenv
  	@echo ">>> Creating venv with: $(PYTHON_BIN)"
  	$(PYTHON_BIN) -m venv $(VENV)

  install: $(VENV)/bin/activate
  	@echo ">>> Installing dependencies"
  	$(PY) -m pip install -U pip
  	$(PIP) install -r ../common/requirements.txt

  run: $(VENV)/bin/activate
  	@echo ">>> Running pipeline"
  	PATH="$(VENV)/bin:$$PATH" \
  	ENSEMBLE_METHOD=tmean \
  	TMEAN_ALPHA=0.1 \
  	bash ../common/scripts/process.sh "../input/$(AUDIO)"

  clean:
  	rm -rf $(VENV)
#+end_src

* WSL2 用環境（env/wsl2）
** Makefile
#+begin_src makefile :tangle env/wsl2/Makefile
  VENV := .venv
  PY   := $(VENV)/bin/python
  PIP  := $(VENV)/bin/pip
  AUDIO ?= foo.wav

  MSCORE_BIN := /usr/bin/mscore
  export MSCORE_BIN

  all: install

  install:
  	sudo apt update
  	sudo apt install -y ffmpeg libsndfile1 musescore
  	python3 -m venv $(VENV)
  	source $(VENV)/bin/activate
  	$(PIP) install -U pip
  	$(PIP) install -r ../common/requirements.txt

  run:
  	source $(VENV)/bin/activate
  	MODELS="htdemucs htdemucs_6s" \
  	ENSEMBLE_METHOD=median \
  	bash ../common/scripts/process.sh "../input/$(AUDIO)"
#+end_src

* Docker 用環境（省略・LilyPond パッケージ入りイメージを使う）
** Dockerfile
#+begin_src dockerfile :tangle env/docker/Dockerfile
  FROM python:3.10-slim

  RUN apt-get update && apt-get install -y ffmpeg libsndfile1 libgl1 && \
      apt-get clean && rm -rf /var/lib/apt/lists/*

  WORKDIR /opt/music2score

  # 共通依存のみコピー
  COPY env/common/requirements.txt ./requirements.txt
  RUN pip install -U pip && pip install -r requirements.txt

  # 共通スクリプトのみコピー（ソースはイメージ内に固定）
  COPY env/common/scripts ./env/common/scripts
  RUN chmod +x env/common/scripts/process.sh

  # 入力音源はコンテナ外から /input にマウントする前提
  ENTRYPOINT ["/opt/music2score/env/common/scripts/process.sh"]
#+end_src

** Makefile
#+begin_src makefile :tangle env/docker/Makefile
  IMAGE := music2score
  AUDIO ?= foo.wav

  build:
  	docker build -t $(IMAGE) -f Dockerfile ../..

  run:
  	docker run --rm \
  	  -v ../../input:/input \
  	  -v ../../out:/out \
  	  $(IMAGE) /input/$(AUDIO)
#+end_src

** docker-compose.yml
#+begin_src yaml :tangle env/docker/docker-compose.yml
  services:
    music2score:
      build:
        context: ../..
        dockerfile: env/docker/Dockerfile
      container_name: music2score
      volumes:
        - ../../input:/input
        - ../../out:/out
      command: ["/opt/music2score/env/common/scripts/process.sh", "/input/foo.wav"]
#+end_src

* 使い方メモ

1. Org を保存後、M-x =org-babel-tangle= でファイル生成
2. macOS:
   - =cd env/macOS=
   - =make install=
   - =make run AUDIO=ここはどこなのか.wav=
3. WSL2:
   - apt install lilypond
   - =make run=
4. Docker:
   - LilyPond を含むイメージをビルド、実行
5. 出力:
   - =out/score/score.pdf=

PDF は LilyPond により高品質に出力されます。
* Developer Guide

** データフロー（directory-level）
#+begin_src mermaid
  flowchart LR
    A["input/foo.wav"] --> B["tmp_audio"]
    B --> C["stems"]
    C --> D["stems_ens"]
    D --> E["midi"]
    D --> F["ASR src"]
    F --> G["lyrics JSON"]
    E --> H["MusicXML"]
    G --> H
    H --> I["PDF"]
#+end_src

** シーケンス図（macOS 例）
#+begin_src mermaid
  sequenceDiagram participant User participant Make participant SH as
    "process.sh" participant Dem as "demucs" participant Ens as
    "ensemble" participant BP as "basic-pitch" participant ASR as "ASR"
    participant XML as "XML builder" participant MS as "mscore"

    User->>Make: make run Make->>SH: process.sh foo.wav SH->>Dem: run
    demucs (multi-model) Dem-->>SH: stems SH->>Ens: ensemble_stems.py
    Ens-->>SH: fused stems loop stems SH->>BP: basic-pitch BP-->>SH:
    midi end SH->>ASR: asr.py ASR-->>SH: lyrics_words.json SH->>XML:
    align_musicxml.py XML-->>SH: score.musicxml SH->>MS: mscore -o
    score.pdf MS-->>SH: score.pdf SH-->>User: DONE
#+end_src

* 拡張ポイント

** 追加しやすい箇所（表）
| 項目             | 変更地点                                        | 備考                      |
|-----------------+------------------------------------------------+--------------------------|
| Demucs モデル追加 | Makefile の MODELS                             | ensemble_stems.py 修正不要 |
| アンサンブル手法   | ENSEMBLE_METHOD                                 | median/tmean             |
| ステム種追加      | ensemble_stems.py, process.sh, align_musicxml.py | 3 箇所必須                |
| 歌詞推定モデル変更 | asr.py                                         | Whisper variant など      |
| 拍子・テンポ可変   | align_musicxml.py                               | 3/4, ritardando 等       |

* トラブルシュート
** Demucs が落ちる
- モデルを軽量化
- 正規化のサンプリングレート変更

** Basic Pitch が MIDI を出さない
- ログ有効化
- 無音区間が長い可能性

** lyrics_words.json が空
- ASR_SRC の WAV を確認

** MusicXML が生成されない
- Basic Pitch の *_basic_pitch.mid があるか確認

* 使い方（User 向け）
1. Org 保存 → =org-babel-tangle=
2. macOS:
   #+begin_src sh
     cd env/macOS
     make install
     make run
   #+end_src
3. WSL2:
   #+begin_src sh
     cd env/wsl2
     make install
     make run
   #+end_src
4. Docker:
   #+begin_src sh
     cd env/docker make build make run
   #+end_src

* Appendix: music2score 開発ポリシー
- 共通ロジックは *env/common* に集約
- 環境依存は *env/macOS, env/wsl2, env/docker*
- Mermaid 図は org-babel-preview で視覚化
- tangle → 再現性のあるビルド

以上で、開発者が迷わずコードを追跡できる README.org 完全版となります。
